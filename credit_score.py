# -*- coding: utf-8 -*-
"""credit score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XDMislaM5y-aR5cvCkd36M6jkBj95VAA

# **Importing Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load Data
train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

# Combine train and test for consistent preprocessing
train_data['source'] = 'train'
test_data['source'] = 'test'
data = pd.concat([train_data, test_data], ignore_index=True)

# Data Exploration (EDA)
print(data.info())
print(data.describe())

"""# **Exploratory Data Analysis**"""

#Visualize distributions of numerical features
numerical_features = data.select_dtypes(include=['float64', 'int64']).columns
for col in numerical_features:
    plt.figure(figsize=(6, 4))
    sns.histplot(data[col], kde=True, bins=30, color='blue')
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

# Boxplots for detecting outliers
for col in numerical_features:
    plt.figure(figsize=(6, 4))
    sns.boxplot(data[col], color='green')
    plt.title(f"Boxplot of {col}")
    plt.xlabel(col)
    plt.show()



# Correlation heatmap
plt.figure(figsize=(12, 8))
# Include only numerical features for correlation calculation
correlation_matrix = data.select_dtypes(include=['number']).corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Pairplot for numerical features
sample_data = data.sample(frac=0.1, random_state=42)  # Sampling for better visualization
sns.pairplot(sample_data[numerical_features], diag_kind='kde', corner=True)
plt.suptitle("Pairplot of Numerical Features", y=1.02)
plt.show()

"""# **Data Preprocessing**"""

# Handling missing values
# Fill numeric columns with median and categorical columns with mode
for col in data.select_dtypes(include=['float64', 'int64']).columns:
    data[col].fillna(data[col].median(), inplace=True)
for col in data.select_dtypes(include=['object']).columns:
    data[col].fillna(data[col].mode()[0], inplace=True)

# Handle zeros in numerical columns if they are not logical
columns_with_zeros = ['Monthly_Inhand_Salary', 'Credit_Utilization_Ratio', 'Monthly_Balance']
for col in columns_with_zeros:
    # Convert the column to numeric, handling errors by coercing to NaN
    data[col] = pd.to_numeric(data[col], errors='coerce')
    # Replace zeros with the median, ignoring NaNs if any
    data[col] = data[col].replace(0, data[col].median())

# Outlier Detection and Handling
# ... (rest of the code remains the same)

# Outlier Detection and Handling
for col in ['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Outstanding_Debt']:
    # Convert the column to numeric before calculating quantiles
    data[col] = pd.to_numeric(data[col], errors='coerce')
    q1 = data[col].quantile(0.25)
    q3 = data[col].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    data[col] = np.where(data[col] < lower_bound, lower_bound, data[col])
    data[col] = np.where(data[col] > upper_bound, upper_bound, data[col])

# Encoding categorical variables
le = LabelEncoder()
for col in data.select_dtypes(include=['object']).columns:
    if col not in ['source', 'Credit_Score']:
        data[col] = le.fit_transform(data[col])

"""# **Feature Engineering**"""

# Feature Engineering
# 1. Create Debt-to-Income Ratio
data['Debt_to_Income_Ratio'] = data['Outstanding_Debt'] / (data['Annual_Income'] + 1e-5)

# 2. Create Payment Delay Ratio
data['Payment_Delay_Ratio'] = data['Num_of_Delayed_Payment'] / (data['Num_of_Loan'] + 1)

# 3. Categorize Age Groups
bins = [0, 25, 40, 60, 100]
labels = ['Young', 'Adult', 'Middle-Aged', 'Senior']
data['Age_Group'] = pd.cut(data['Age'], bins=bins, labels=labels)
data['Age_Group'] = le.fit_transform(data['Age_Group'])

# 4. Credit Utilization Category
data['Credit_Utilization_Category'] = pd.cut(data['Credit_Utilization_Ratio'], bins=[0, 0.3, 0.6, 1.0], labels=['Low', 'Medium', 'High'])
data['Credit_Utilization_Category'] = le.fit_transform(data['Credit_Utilization_Category'])

# Extracting 'train' and 'test' data after preprocessing
train_data = data[data['source'] == 'train'].drop(['source'], axis=1)
test_data = data[data['source'] == 'test'].drop(['source', 'Credit_Score'], axis=1)

# Separating features and target
X = train_data.drop(['Credit_Score', 'ID', 'Customer_ID', 'Name', 'SSN'], axis=1)
y = train_data['Credit_Score']

# Encode the target variable 'y' before splitting
le = LabelEncoder()
y = le.fit_transform(y)  # Transform 'Good', 'Poor', 'Standard' to 0, 1, 2

# Splitting into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Check and handle NaNs before scaling
print("NaNs in X_train before handling:", np.isnan(X_train).sum()) # Check for NaNs in X_train
print("NaNs in X_val before handling:", np.isnan(X_val).sum()) # Check for NaNs in X_val

# Option 1: Impute NaNs with median
# from sklearn.impute import SimpleImputer
# imputer = SimpleImputer(strategy='median') # Replace with your preferred strategy
# X_train = imputer.fit_transform(X_train)
# X_val = imputer.transform(X_val)

# Option 2: Drop rows with NaNs
# Reset index of X_train and X_val before dropping rows
X_train = X_train.reset_index(drop=True)
X_val = X_val.reset_index(drop=True)

X_train = X_train.dropna()
X_val = X_val.dropna()

# Now drop corresponding rows from y_train and y_val using boolean indexing
y_train = y_train[X_train.index]
y_val = y_val[X_val.index]


# Scaling numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

#Drop the columns in test_data before scaling
test_data = test_data.drop(['ID', 'Customer_ID', 'Name', 'SSN'], axis=1) # Drop the problematic columns from test_data
test_data = scaler.transform(test_data) # Now transform the test_data

"""# **Model building**"""

# Model Selection and Training
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(random_state=42)
}

"""# **Model Evaluation**"""

results = {}
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    print(f"{model_name} Accuracy: {accuracy:.4f}")
    print(classification_report(y_val, y_pred))
    results[model_name] = accuracy

# Selecting the best model
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]
print(f"Best Model: {best_model_name}")

"""# **Model Improvement**"""

# Random Forest Model Improvement

from sklearn.model_selection import RandomizedSearchCV
if best_model_name == "Random Forest":
    # Hyperparameter tuning using RandomizedSearchCV
    param_distributions = {
        'n_estimators': [50, 100, 150],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    randomized_search = RandomizedSearchCV(estimator=best_model, param_distributions=param_distributions, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)
    randomized_search.fit(X_train, y_train)

    # Update the best model with tuned parameters
    best_model = randomized_search.best_estimator_
    print(f"Best Random Forest Parameters: {randomized_search.best_params_}")

# Retrain the best model
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)
print(f"Improved Random Forest Accuracy: {accuracy:.4f}")
print(classification_report(y_val, y_pred))

# Feature Importance for Random Forest
feature_importances = best_model.feature_importances_
features = X.columns
feature_df = pd.DataFrame({"Feature": features, "Importance": feature_importances})
feature_df.sort_values(by="Importance", ascending=False, inplace=True)

plt.figure(figsize=(10, 6))
sns.barplot(x="Importance", y="Feature", data=feature_df)
plt.title("Feature Importance")
plt.show()

# Making predictions on the test data
test_predictions = best_model.predict(test_data)

# Predict Credit Score for a new input example
example_input = {
    'Age': 35,
    'Annual_Income': 60000,
    'Monthly_Inhand_Salary': 5000,
    'Num_Bank_Accounts': 3,
    'Num_Credit_Card': 2,
    'Interest_Rate': 10,
    'Num_of_Loan': 1,
    'Delay_from_due_date': 5,
    'Num_of_Delayed_Payment': 2,
    'Changed_Credit_Limit': 1,
    'Num_Credit_Inquiries': 2,
    'Credit_Mix': 2,
    'Outstanding_Debt': 15000,
    'Credit_Utilization_Ratio': 30,
    'Credit_History_Age': 10,
    'Payment_of_Min_Amount': 1,
    'Total_EMI_per_month': 1000,
    'Amount_invested_monthly': 500,
    'Payment_Behaviour': 3,
    'Monthly_Balance': 1000,
    # Add placeholders for engineered features
    'Month': 6,  # Example: June
    'Occupation': 1,  # Example: Self-Employed (assuming LabelEncoded)
    'Type_of_Loan': 2,  # Example: Personal Loan (assuming LabelEncoded)
    # Add other features with placeholders (replace with actual values)
    'Num_Credit_Inquiries_Last_6_Months': 2,  # Hypothetical feature
    'Avg_Credit_Card_Utilization_Last_Year': 0.25,  # Hypothetical feature
    'Num_Late_Payments_Last_12_Months': 1,  # Hypothetical feature
    'Has_Active_Loan': 1,  # Hypothetical binary feature (1 for Yes, 0 for No)
    # ... add other relevant features with placeholders
}

# Convert input to DataFrame
example_df = pd.DataFrame([example_input])


# Perform feature engineering steps as before to add the missing columns
# 1. Create Debt-to-Income Ratio
example_df['Debt_to_Income_Ratio'] = example_df['Outstanding_Debt'] / (example_df['Annual_Income'] + 1e-5)

# 2. Create Payment Delay Ratio
example_df['Payment_Delay_Ratio'] = example_df['Num_of_Delayed_Payment'] / (example_df['Num_of_Loan'] + 1)

# ... (previous code) ...

# 3. Categorize Age Groups
bins = [0, 25, 40, 60, 100]
labels = ['Young', 'Adult', 'Middle-Aged', 'Senior']
example_df['Age_Group'] = pd.cut(example_df['Age'], bins=bins, labels=labels)

# Check if 'Adult' is in le.classes_, and add it if not
if 'Adult' not in le.classes_:
    le.classes_ = np.append(le.classes_, 'Adult')

example_df['Age_Group'] = le.transform(example_df['Age_Group'])  # Use the same LabelEncoder instance 'le'

# ... (rest of the code) ...
# 4. Credit Utilization Category
# ... (Use the same bins and labels as before) ...
example_df['Credit_Utilization_Category'] = pd.cut(example_df['Credit_Utilization_Ratio'], bins=[0, 0.3, 0.6, 1.0], labels=['Low', 'Medium', 'High'], include_lowest=True, duplicates='drop')

# Handle NaN values if any:
example_df['Credit_Utilization_Category'] = example_df['Credit_Utilization_Category'].cat.add_categories('Unknown').fillna('Unknown')

example_df['Credit_Utilization_Category'] = le.fit_transform(example_df['Credit_Utilization_Category']) # Use the same LabelEncoder instance 'le'


# Ensure example_df has the same columns as X_train, in the same order
example_df = example_df[X.columns]  # Reorder columns to match X_train

# Now scale the data
example_df = scaler.transform(example_df)

# Predict credit score for the input
predicted_credit_score = best_model.predict(example_df)
print(f"Predicted Credit Score for the input: {predicted_credit_score[0]}")
if predicted_credit_score[0] == 0:
  print("Credit Score: Poor")
elif predicted_credit_score[0] == 1:
  print("Credit Score: Standard")
else:
  print("Credit Score: Good")

"""# **Interpretation of credit score**

1.   '0' represents Bad credit score
2.   '1' represents Standard score
3.   '2' represents Good score








"""

!pip install joblib
import joblib

# Save the model and scaler
import joblib
joblib.dump(best_model, 'credit_score_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

# Previous lines...
print("Training features count:", X_train.shape[1])
print("Prediction features count:", example_df.shape[1]) # Changed to example_df
# Subsequent lines...

